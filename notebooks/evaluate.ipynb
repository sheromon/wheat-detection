{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9080008f",
   "metadata": {},
   "source": [
    "This notebook runs evaluation on the validation set using a saved checkpoint. Predictions and ground truth annotation data can be saved to files and loaded later for in-depth analysis and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360cdb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import brambox as bb\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "\n",
    "from wheat.config import load_config\n",
    "from wheat.data_module import WheatDataModule\n",
    "from wheat.scripts import evaluate\n",
    "from wheat import visualization as vis\n",
    "\n",
    "pd.options.plotting.backend = 'plotly'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49919bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bff952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# move from 'notebooks' directory to top level directory\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa3dfc6",
   "metadata": {},
   "source": [
    "## Running inference and evaluation\n",
    "PyTorch Lightning makes it easy to run evaluation/validation on the val set using saved weights. We're going to be sneaky here and save off the predicted detections and the ground truth data so that we can do custom analysis later. (The PyTorch Lightning validation process doesn't have an obvious way to return the data, thus the sneakiness using EVs below.)\n",
    "\n",
    "If you have already run evaluation and saved the results previously, you can just run the first cell and skip the rest of this section to load the previous results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a2b62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = Path('lightning_logs/kaggle_version_3')\n",
    "checkpoint_path = output_dir/'epoch=9-step=6849.ckpt'\n",
    "\n",
    "# load a configuration file\n",
    "config = load_config('wheat/config/config.ini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79c4e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set pytorch lightning flags here\n",
    "pl_args_dict = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2676b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if this environment variable is set, detections and ground truth annotations\n",
    "# will be saved to .csv files for easy loading and analysis later on\n",
    "os.environ['CMD_WHEAT_OUTPUT_DIR'] = str(output_dir)\n",
    "evaluate.evaluate(config, pl_args_dict, checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795e4a02",
   "metadata": {},
   "source": [
    "## Loading saved detections and annotations\n",
    "Since the evaluation outputs have been saved to disk in the output directory, we can load the detections and ground truth annotations at any time later without having to rerun inference. Annotation and detection data are saved in a format that is compatible with the brambox Python package. Annotation and detection dataframes have a similar format, except that detection data includes a 'confidence' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438037fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "det_df = pd.read_csv(output_dir/'det.csv', index_col=0)\n",
    "det_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b2ec67",
   "metadata": {},
   "outputs": [],
   "source": [
    "anno_df = pd.read_csv(output_dir/'anno.csv', index_col=0)\n",
    "anno_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e409f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pr_curve(det_df, anno_df, iou_threshold):\n",
    "    \"\"\"Plot a precision-recall curve using the specified IOU threshold.\"\"\"\n",
    "    df_pr = bb.stat.pr(det_df, anno_df, threshold=iou_threshold)\n",
    "    df_pr = df_pr.append({'precision': 0, 'recall': df_pr['recall'].max(), 'confidence': 0}, ignore_index=True)\n",
    "    ap = bb.stat.ap(df_pr)\n",
    "    fig = df_pr.plot('recall', 'precision', title=f'AP at IOU {iou_threshold}: {ap:.3f}')\n",
    "    fig.update_xaxes(range=[0, 1])\n",
    "    fig.update_yaxes(range=[0, 1])\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee48e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can plot a pr curve for the entire validation dataset\n",
    "# later, we plot a pr curve for a single image\n",
    "plot_pr_curve(det_df, anno_df, iou_threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333004ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function calculates the ap for each invidiual image\n",
    "def get_per_image_ap_values(det_df, anno_df, iou_thresholds):\n",
    "    images = det_df.image.unique()\n",
    "    data_dict = {'image': images}\n",
    "    for iou_threshold in iou_thresholds:\n",
    "        image_ap_vals = []\n",
    "        for image in images:\n",
    "            pr_image = bb.stat.pr(\n",
    "                det_df[det_df.image == image],\n",
    "                anno_df[anno_df.image == image],\n",
    "                threshold=iou_threshold)\n",
    "            ap = bb.stat.ap(pr_image)\n",
    "            image_ap_vals.append(ap)\n",
    "        ap_str = 'ap' + str(round(100 * iou_threshold))\n",
    "        data_dict[ap_str] = image_ap_vals\n",
    "    return pd.DataFrame(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef2e9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_ap_df = get_per_image_ap_values(det_df, anno_df, iou_thresholds=[0.5, 0.75])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd9fbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code adds the number of ground truth annotations for each image as a new column\n",
    "image_ap_df = image_ap_df.merge(\n",
    "    anno_df['image'].value_counts().rename('num_annos'),\n",
    "    how='left', left_on='image', right_index=True,\n",
    ")\n",
    "image_ap_df['num_annos'] = image_ap_df['num_annos'].fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e243ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by ap75 and reset the index\n",
    "image_ap_df = image_ap_df.sort_values('ap75').reset_index(drop=True)\n",
    "image_ap_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f394c6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_ap_df.plot.scatter(x=image_ap_df.index, y=['ap50', 'ap75'], hover_data=['image', 'num_annos'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31efa6e7",
   "metadata": {},
   "source": [
    "We can use the information on which images had the best or worst AP values to plot the images with their ground truth bounding boxes and predicted detections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615816ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_image_with_detections(dataset, image_index, det_df=None):\n",
    "    image, labels = dataset[image_index]\n",
    "    # plot ground truth bounding boxes in blue\n",
    "    result = draw_bounding_boxes(\n",
    "        vis.image_float_to_int_transform(image), labels['boxes'], colors='blue', width=5)\n",
    "    # plot predicted bounding boxes in yellow\n",
    "    if det_df is not None:\n",
    "        det_df_filtered = det_df[det_df.image == image_index]\n",
    "        boxes = det_df_filtered[['x_top_left', 'y_top_left', 'width', 'height']].values\n",
    "        boxes[:, 2:] += boxes[:, :2]\n",
    "        scores = det_df_filtered['confidence'].round(2).astype(str).values.tolist()\n",
    "        result = draw_bounding_boxes(\n",
    "            result, torch.tensor(boxes), labels=scores, colors='yellow', width=5, \n",
    "            font='DejaVuSans.ttf', font_size=20)\n",
    "    vis.show(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86566114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the dataset\n",
    "wheat_data_module = WheatDataModule(config)\n",
    "wheat_data_module.setup(stage='validate')\n",
    "val_dataset = wheat_data_module.val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20816bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here's an image with AP50 and AP75 at zero\n",
    "plt.rcParams['figure.figsize'] = [10, 10]\n",
    "display_image_with_detections(val_dataset, 213, det_df[det_df.confidence>0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874dd2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here's an image with very low AP75 but pretty decent AP50\n",
    "display_image_with_detections(val_dataset, 64, det_df[det_df.confidence>0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befa38e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pr_curve(det_df[det_df.image==64], anno_df[anno_df.image==64], iou_threshold=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23727190",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pr_curve(det_df[det_df.image==64], anno_df[anno_df.image==64], iou_threshold=0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
